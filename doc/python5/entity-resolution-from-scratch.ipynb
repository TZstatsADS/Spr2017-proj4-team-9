{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Clean data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import codecs\n",
        "import pandas as pd\n",
        "import itertools\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rexa_dir = '/home/is2548/ML/entity-resolution/data/'\n",
        "field_names = [\n",
        "    'author-in-focus', 'author-in-focus-score',\n",
        "    'authorlist', 'alt-authorlist', 'altTitle', \n",
        "    'editor', 'email', 'institution', 'journal',\n",
        "    'abstract', 'body', 'keyword', 'title', 'year'\n",
        "]\n",
        "authors_f = []\n",
        "\n",
        "for root, dirs, files in os.walk(rexa_dir, topdown=False):\n",
        "    for f in files:\n",
        "        if f.endswith('.txt'):\n",
        "            authors_f.append({\n",
        "                'path': '{}/{}'.format(root, f),\n",
        "                'cluster_name': root.split('/')[-1]\n",
        "            })\n",
        "print(authors_f[0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def raw2dict(raw):\n",
        "    return { entry.split(':', 1)[0].strip():entry.split(':', 1)[1].strip() for entry in raw if entry.strip() != ''}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "def clean_name(xml_string):\n",
        "    root = ET.fromstring(xml_string)\n",
        "    return ' '.join([child.text for child in root])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_na(author):\n",
        "    for field in field_names:\n",
        "        if author.get(field, None) is None:\n",
        "            author[field] = None\n",
        "    return author"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "authors_d = []\n",
        "\n",
        "for author_f in authors_f:\n",
        "    with codecs.open(author_f['path'], 'r', 'utf-8', errors='ignore') as f:\n",
        "        author_raw = raw2dict(f.readlines())\n",
        "        author_raw['cluster_name'] = author_f['cluster_name']\n",
        "        authors_d.append(author_raw)\n",
        "print(len(authors_d))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean author data\n",
        "authors_dict = {}\n",
        "\n",
        "for index, author_d in enumerate(authors_d):\n",
        "    fill_na(author_d)\n",
        "    author_d['author-in-focus'] = clean_name(author_d['author-in-focus'])\n",
        "    author_d['authorlist'] = tuple(\n",
        "        clean_name(author_name)\n",
        "        for author_name in author_d['authorlist'].split('%%')\n",
        "        if clean_name(author_name) != author_d['author-in-focus']\n",
        "    ) or tuple([''])\n",
        "    if author_d.get('alt-authorlist', None) is not None:\n",
        "        author_d['alt-authorlist'] = tuple(\n",
        "            clean_name(author_name)\n",
        "            for author_name in author_d['alt-authorlist'].split('%%')\n",
        "            if clean_name(author_name) != author_d['author-in-focus']\n",
        "        ) or tuple([''])\n",
        "    if author_d.get('keyword', None) is not None:\n",
        "        author_d['keyword'] = tuple([keyword.strip() for keyword in author_d['keyword'].split(',') ])\n",
        "    \n",
        "    if author_d.get('journal', None) is None:\n",
        "        author_d['journal'] = '';\n",
        "        \n",
        "    if author_d.get('author-in-focus-score', None) is not None:\n",
        "        author_d['author-in-focus-score'] = float(author_d['author-in-focus-score'])\n",
        "    authors_dict[index] = author_d\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(authors_d)\n",
        "df.describe()\n",
        "df = df[[\n",
        "    'author-in-focus', 'author-in-focus-score', 'authorlist', 'title',\n",
        "    'alt-authorlist', 'journal', 'abstract', 'body', 'institution',\n",
        "    'email', 'altTitle', 'keyword', 'editor', 'year', 'cluster_name'\n",
        "]]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff = 1000\n",
        "df1 = df[: cutoff]\n",
        "df2 = df[cutoff: cutoff + cutoff / 2]\n",
        "df2 = df2.reset_index()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run with dedupe"
      ],
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from itertools import groupby\n",
        "# training_data = dict([(key, tuple(group)) for key, group in groupby(authors_d, lambda item: item['cluster_name'])])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dedupe\n",
        "\n",
        "# fields = [\n",
        "#     {'field' : 'author-in-focus', 'type': 'String'},\n",
        "# #     {'field' : 'author-in-focus-score', 'type': 'Price'},\n",
        "#     {'field' : 'authorlist', 'type': 'Set'},\n",
        "#     {'field' : 'alt-authorlist', 'type': 'Set', 'has missing' : True},\n",
        "#     {'field' : 'email', 'type': 'String', 'has missing' : True},\n",
        "#     {'field' : 'keyword', 'type': 'Set', 'has missing' : True},\n",
        "# #     {'field' : 'abstract', 'type': 'Text', 'has missing' : True},\n",
        "# #     {'field' : 'body', 'type': 'Text', 'has missing' : True},\n",
        "#     {'field' : 'journal', 'type': 'String', 'has missing' : True},\n",
        "#     {'field' : 'institution', 'type': 'String', 'has missing' : True},\n",
        "# ]\n",
        "\n",
        "# deduper = dedupe.Dedupe(fields)\n",
        "\n",
        "# deduper.sample(authors_dict)\n",
        "\n",
        "# match_clusters = [\n",
        "#     'SarahJRussell',\n",
        "#     'DMAllen-ohu',\n",
        "#     'AlvinBlum',\n",
        "#     'SJonesKnowEng',\n",
        "#     'MAJordan',\n",
        "#     'DKoller',\n",
        "#     'LHLee-elec',\n",
        "#     'JGMcGuire',\n",
        "#     'AlanMoore',\n",
        "#     'RajeevMotwani',\n",
        "#     'SebastianThrun',\n",
        "#     'StephenJYoung'\n",
        "# ]\n",
        "\n",
        "# distinct_clusters = [\n",
        "#     ('SAYoung', 'SCKYoung'),\n",
        "#     ('SAYoung', 'SCKYoung'),\n",
        "#     ('StephenRussell', 'StephenRussellBIO'),\n",
        "#     ('RajeevMotwani', 'RaviMotwani'),\n",
        "#     ('AndrewJMoore', 'AndrewMMoore'),\n",
        "#     ('JBMcGuire', 'JGMcGuire'),\n",
        "#     ('LALee1', 'LALee2'),\n",
        "#     ('DKoller', 'DanielKoller'),\n",
        "#     ('MarilynJordan', 'MauriceJordan'),\n",
        "#     ('SCJones1', 'SCJones2'),\n",
        "#     ('AlvinBlum', 'AvrimBlum'),\n",
        "#     ('DAllen-jr', 'DAllen-ucla'),\n",
        "# ]\n",
        "\n",
        "# deduper.markPairs({\n",
        "#     'match': [\n",
        "#         (training_data.get(cluster_name)[-1], training_data.get(cluster_name)[-2])\n",
        "#         for cluster_name in match_clusters\n",
        "#     ],\n",
        "#     'distinct': [\n",
        "#         (training_data.get(cluster_l)[0], training_data.get(cluster_r)[0])\n",
        "#         for cluster_l, cluster_r in distinct_clusters\n",
        "#     ]\n",
        "# })\n",
        "\n",
        "# print('start training...')\n",
        "\n",
        "# deduper.train()\n",
        "\n",
        "# print('finished...')\n",
        "\n",
        "# training_file = 'author_training.json'\n",
        "\n",
        "# with open(training_file, 'w') as tf:\n",
        "#     deduper.writeTraining(tf)\n",
        "    \n",
        "# threshold = deduper.threshold(authors_dict, recall_weight=1)\n",
        "\n",
        "# print('clustering...')\n",
        "# clustered_dupes = deduper.match(authors_dict, threshold)\n",
        "\n# print('# duplicate sets', len(clustered_dupes))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Features"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import jellyfish\n",
        "import pandas as pd\n",
        "\n",
        "pairwise_df_train = None\n",
        "pairwise_df_test = None\n",
        "\n",
        "features = [\n",
        "        'author_damerau_levenshtein_distance', 'author_hamming_distance',\n",
        "        'author_jaro_distance', 'author_jaro_winkler', 'author_levenshtein_distance',\n",
        "        'authorlist_damerau_levenshtein_distance', 'authorlist_hamming_distance',\n",
        "        'authorlist_jaro_distance', 'authorlist_jaro_winkler', 'authorlist_levenshtein_distance',\n",
        "        'title_damerau_levenshtein_distance', 'title_hamming_distance',\n",
        "        'title_jaro_distance', 'title_jaro_winkler', 'title_levenshtein_distance',\n",
        "        'journal_damerau_levenshtein_distance', 'journal_hamming_distance',\n",
        "        'journal_jaro_distance', 'journal_jaro_winkler', 'journal_levenshtein_distance'\n",
        "    ]\n",
        "target = 'identical'\n",
        "\n",
        "pairwise_df_train = pd.DataFrame(\n",
        "    columns=features + [target]\n",
        ")\n",
        "\n",
        "pairwise_df_test = pd.DataFrame(\n",
        "    columns=features + [target]\n",
        ")\n",
        "\n",
        "def creat_row(df, row_1, row_2):\n",
        "    row0 = df.ix[row_1]\n",
        "    row1 = df.ix[row_2]\n",
        "\n\n",
        "    aio0 = row0['author-in-focus']\n",
        "    aio1 = row1['author-in-focus']\n",
        "    \n",
        "    al0 = row0['authorlist']\n",
        "    al1 = row1['authorlist']\n",
        "    \n",
        "    title_0 = row0['title']\n",
        "    title_1 = row1['title']\n",
        "    \n",
        "    journal_0 = row0['journal']\n",
        "    journal_1 = row1['journal']\n",
        "    \n",
        "    cluster_name_0 = row0['cluster_name']\n",
        "    cluster_name_1 = row1['cluster_name']\n",
        "    \n",
        "#     print(int(cluster_name_0 == cluster_name_1))\n",
        "\n",
        "    return [\n",
        "        jellyfish.damerau_levenshtein_distance(aio0.decode('unicode-escape'), aio1.decode('unicode-escape')),\n",
        "        jellyfish.hamming_distance(aio0.decode('unicode-escape'), aio1.decode('unicode-escape')),\n",
        "        jellyfish.jaro_distance(aio0.decode('unicode-escape'), aio1.decode('unicode-escape')),\n",
        "        jellyfish.jaro_winkler(aio0.decode('unicode-escape'), aio1.decode('unicode-escape')),\n",
        "        jellyfish.levenshtein_distance(aio0.decode('unicode-escape'), aio1.decode('unicode-escape')),\n",
        "#         jellyfish.match_rating_comparison(aio0.decode('unicode-escape'), aio1.decode('unicode-escape')),\n",
        "        min([jellyfish.damerau_levenshtein_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1]),\n",
        "        min([jellyfish.hamming_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1]),\n",
        "        min([jellyfish.jaro_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1]),\n",
        "        min([jellyfish.jaro_winkler(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1]),\n",
        "        min([jellyfish.levenshtein_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1]),\n",
        "#         int(max([jellyfish.match_rating_comparison(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1])),\n",
        "        jellyfish.damerau_levenshtein_distance(title_0.decode('unicode-escape'), title_1.decode('unicode-escape')),\n",
        "        jellyfish.hamming_distance(title_0.decode('unicode-escape'), title_1.decode('unicode-escape')),\n",
        "        jellyfish.jaro_distance(title_0.decode('unicode-escape'), title_1.decode('unicode-escape')),\n",
        "        jellyfish.jaro_winkler(title_0.decode('unicode-escape'), title_1.decode('unicode-escape')),\n",
        "        jellyfish.levenshtein_distance(title_0.decode('unicode-escape'), title_1.decode('unicode-escape')),\n",
        "\n",
        "        jellyfish.damerau_levenshtein_distance(journal_0.decode('unicode-escape'), journal_1.decode('unicode-escape')),\n",
        "        jellyfish.hamming_distance(journal_0.decode('unicode-escape'), journal_1.decode('unicode-escape')),\n",
        "        jellyfish.jaro_distance(journal_0.decode('unicode-escape'), journal_1.decode('unicode-escape')),\n",
        "        jellyfish.jaro_winkler(journal_0.decode('unicode-escape'), journal_1.decode('unicode-escape')),\n",
        "        jellyfish.levenshtein_distance(journal_0.decode('unicode-escape'), journal_1.decode('unicode-escape')),\n",
        "\n",
        "        int(cluster_name_0 == cluster_name_1)\n",
        "    ]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# row0 = df.ix[0]\n",
        "# row1 = df.ix[1]\n",
        "\n",
        "# aio0 = row0['author-in-focus']\n",
        "# aio1 = row1['author-in-focus']\n",
        "\n",
        "# al0 = row0['authorlist']\n",
        "# al1 = row1['authorlist']\n",
        "# print min([jellyfish.damerau_levenshtein_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1])\n",
        "# print min([jellyfish.hamming_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1])\n",
        "# print min([jellyfish.jaro_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1])\n",
        "# print min([jellyfish.jaro_winkler(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1])\n",
        "# print min([jellyfish.levenshtein_distance(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1])\n",
        "# [jellyfish.match_rating_comparison(x.decode('unicode-escape'), y.decode('unicode-escape')) for x in al0 for y in al1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (n1, n2) in enumerate(itertools.product(range(len(df1)), range(len(df1)))):\n",
        "    if i % 1000 == 0 or i % 1000 == 999:\n",
        "        print i, n1, n2\n",
        "    if n1 != n2:\n",
        "        pairwise_df_train.loc[i] = creat_row(df1, n1, n2)\n",
        "        \n",
        "pairwise_df_train.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (n1, n2) in enumerate(itertools.product(range(len(df2)), range(len(df2)))):\n",
        "    if i % 1000 == 0 or i % 1000 == 999:\n",
        "        print i, n1, n2\n",
        "    if n1 != n2:\n",
        "        pairwise_df_test.loc[i] = creat_row(df2, n1, n2)\n",
        "\npairwise_df_test.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairwise_df_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_df_train['identical'].unique()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pairwise_df_train[features].values\n",
        "y_train = pairwise_df_train[target].values\n",
        "\n",
        "X_test = pairwise_df_test[features].values\n",
        "y_test = pairwise_df_test[target].values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "language": "python",
      "display_name": "Python 2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "kernel_info": {
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}