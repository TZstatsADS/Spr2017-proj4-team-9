{"distinct": [{"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["I J Barton", "A J Prata", "I G Watterson", "S A Young"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["I J Barton", "A J Prata", "I G Watterson", "S A Young"]}, "institution": null, "title": "Identification of the Mount Hudson Volcanic Cloud over SE Australia\",", "journal": "Geophysical Research Lettersyear: 1992", "abstract": null, "author-in-focus": "S A Young", "cluster_name": "SAYoung", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.97744435, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["S C K Young", "C Ian McGibbon"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["S C K Young", "C Ian McGibbon"]}, "institution": null, "title": "The Control System of the Datapac Network", "journal": null, "abstract": null, "author-in-focus": "S C K Young", "cluster_name": "SCKYoung", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.96992576, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["I J Barton", "A J Prata", "I G Watterson", "S A Young"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["I J Barton", "A J Prata", "I G Watterson", "S A Young"]}, "institution": null, "title": "Identification of the Mount Hudson Volcanic Cloud over SE Australia\",", "journal": "Geophysical Research Lettersyear: 1992", "abstract": null, "author-in-focus": "S A Young", "cluster_name": "SAYoung", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.97744435, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["S C K Young", "C Ian McGibbon"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["S C K Young", "C Ian McGibbon"]}, "institution": null, "title": "The Control System of the Datapac Network", "journal": null, "abstract": null, "author-in-focus": "S C K Young", "cluster_name": "SCKYoung", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.96992576, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Gernot Heiser", "Kevin Elphinstone", "Jerry Vochteloo", "Stephen Russell", "Jochen Liedtke"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Gernot Heiser", "Kevin Elphinstone", "Jerry Vochteloo", "Stephen Russell", "Jochen Liedtke"]}, "institution": null, "title": "The Mungi Single-Address-Space Operating System", "journal": null, "abstract": null, "author-in-focus": "Stephen Russell", "cluster_name": "StephenRussell", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.6251166, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["S Patel", "J Rabone", "S Russell", "J Tissen", "W Klaffke"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["S Patel", "J Rabone", "S Russell", "J Tissen", "W Klaffke"]}, "institution": null, "title": "Iterated reaction graphs: simulating complex Maillard reaction pathways", "journal": "J. Chem. Inf. Comput. Sci.,year: 2001", "abstract": null, "author-in-focus": "S Russell", "cluster_name": "StephenRussellBIO", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.6251166, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["R Motwani"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["R Motwani"]}, "institution": null, "title": "Approximation algorithms", "journal": "Stanford Technical Report STAN-CS-92-year: 1992", "abstract": null, "author-in-focus": "R Motwani", "cluster_name": "RajeevMotwani", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Marten van Dijk", "Sebastian Egner", "Ravi Motwani", "Arie G C Koppelaar"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Marten van Dijk", "Sebastian Egner", "Ravi Motwani", "Arie G C Koppelaar"]}, "institution": null, "title": "Simultaneous zero-tailing of parallel concatenated codes", "journal": null, "abstract": null, "author-in-focus": "Ravi Motwani", "cluster_name": "RaviMotwani", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Mike Brunoli", "Masao Hotta", "Felicia James", "Rudy Koch", "Roy McGuffin", "Andrew J Moore"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Mike Brunoli", "Masao Hotta", "Felicia James", "Rudy Koch", "Roy McGuffin", "Andrew J Moore"]}, "institution": null, "title": "Analog intellectual property: now? Or never?", "journal": null, "abstract": null, "author-in-focus": "Andrew J Moore", "cluster_name": "AndrewJMoore", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.97886527, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["R Kleeman", "A Moore"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["R Kleeman", "A Moore"]}, "institution": null, "title": "A theory for the limitation of ENSO predictability due to stochastic atmospheric transients,", "journal": "J. Atmos. Sci.,year: 1997", "abstract": null, "author-in-focus": "A Moore", "cluster_name": "AndrewMMoore", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["J B McGuire"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["J B McGuire"]}, "institution": null, "title": "Study of Exactly Soluble One-Dimensional N-Body Problems,", "journal": "J. Math. Phys.year: 1964", "abstract": null, "author-in-focus": "J B McGuire", "cluster_name": "JBMcGuire", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9956606, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["J G McGuire", "D R Kuokka", "J C Weber", "J M Tenenbaum", "T T Gruber", "G R Olsen"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["J G McGuire", "D R Kuokka", "J C Weber", "J M Tenenbaum", "T T Gruber", "G R Olsen"]}, "institution": null, "title": "SHADE: Technology for knowledge-based collaboration engineering", "journal": "Journal of Concurrent Engineering: Applications and Research (CERA),year: 1993", "abstract": null, "author-in-focus": "J G McGuire", "cluster_name": "JGMcGuire", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9956606, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["R D Jones", "Y C Lee", "S Qian", "C W Barnes", "K R Bisset", "G M Bruce", "G W Flake", "K Lee", "L A Lee", "W C Mead", "M K O'Rourke", "I J Poli", "L E Thodes"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["R D Jones", "Y C Lee", "S Qian", "C W Barnes", "K R Bisset", "G M Bruce", "G W Flake", "K Lee", "L A Lee", "W C Mead", "M K O'Rourke", "I J Poli", "L E Thodes"]}, "keyword": null, "title": "Nonlinear adaptivenetworks: A little theory, a few applications", "journal": "Technical Report LA--UR--91--273,year: 1990", "abstract": null, "author-in-focus": "L A Lee", "cluster_name": "LALee1", "altTitle": null, "email": null, "editor": null, "year": null, "author-in-focus-score": 0.7979153, "institution": "Los Alamos National Laboratory,"}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["W C Mead", "R D Jones", "Y C Lee", "C W Barnes", "G W Flake", "L A Lee", "M K O'Rourke"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["W C Mead", "R D Jones", "Y C Lee", "C W Barnes", "G W Flake", "L A Lee", "M K O'Rourke"]}, "institution": null, "title": "Prediction of chaotic time seires using cnls-net, example: The Mackey-Glass equation", "journal": "condition-Contructive Approximation,year: 1986", "abstract": null, "author-in-focus": "L A Lee", "cluster_name": "LALee2", "altTitle": "Nonlinear Modelling and Forecasting,", "editor": "In M. Casdagli and S. Eubank, editors,", "year": null, "keyword": null, "author-in-focus-score": 0.7979153, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D Bowman", "D Koller", "L F Hodges"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D Bowman", "D Koller", "L F Hodges"]}, "institution": null, "title": "A methodology for the evaluation of travel techniques for immersive virtual environments", "journal": "Virtual Reality: Journal of the Virtual Reality Society,year: 1998", "abstract": null, "author-in-focus": "D Koller", "cluster_name": "DKoller", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Erik Wilde", "Pascal Freiburghaus", "Daniel Koller", "Bernhard Plattner"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Erik Wilde", "Pascal Freiburghaus", "Daniel Koller", "Bernhard Plattner"]}, "institution": null, "title": "A Group and Session Management System for Distributed Multimedia Applications", "journal": null, "abstract": null, "author-in-focus": "Daniel Koller", "cluster_name": "DanielKoller", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Marilyn Jordan", ""]}, "alt-authorlist": null, "institution": null, "title": "ROW: Nitrogen Cycling in Forests: A Macroplot 15N Experiment", "journal": null, "abstract": "Productivity of temperate forests typically is limited by nitrogen supply Nitrogen cycling in forests is linked to regional and global environmental issues, such as groundwater quality and the trace gases composition of the atmosphere. Forests are being fertilized to stimulate forest growth to meet increasing demands for forest products, and are being used as \"living filters\" to remove nitrogen from applied wastewater. Extensive forested areas are receiving excessive additions of nitrogen through nitrogenous components of acid rain. Although general patterns of nitrogen cycling in unimpacted forests are known, little is known about the nitrogen accumulation potential of forests, or the ability of forests to handle chronic excess nitrogen additions. This research is first of all a comparative ecosystem study of the effects of excess nitrogen additions on nitrogen cycling in early, mid- and late-successional forests on Cape Cod, Massachusetts. Secondly, this study will take advantage of a natural experiment inadvertently set up by the Town of Falmouth, in which 24 hectares of forests and successional vegetation are scheduled to be irrigated with wastewater highly enriched with 15N, a stable isotope of nitrogen. The presence of this 15N label will enable isotopic analyses to be used to trace the fate of the wastewater nitrogen added to the Cape Code forests. The research at the Falmouth site began in 1985, when the wastewater treatment plant was still under construction. In the elapsed 2.5 years a baseline study of nitrogen cycling in the forests has been completed. It is expected that research results will be of fundamental ecological importance and applicable to the treatment and use of municipal wastes.", "author-in-focus": "Marilyn Jordan", "cluster_name": "MarilynJordan", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.94425213, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Maurice Jordan"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Maurice Jordan"]}, "institution": null, "title": "APL extensions - a users view", "journal": null, "abstract": null, "author-in-focus": "Maurice Jordan", "cluster_name": "MauriceJordan", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.94425213, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["S C Jones", "W E Rozelle"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["S C Jones", "W E Rozelle"]}, "institution": null, "title": "Graphical techniques for determining relative permeability from displacement experiments", "journal": "Trans. AIME,year: 1978", "abstract": null, "author-in-focus": "S C Jones", "cluster_name": "SCJones1", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9755763, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D H Petley", "S C Jones"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D H Petley", "S C Jones"]}, "institution": null, "title": "\"Thermal Management for a Mach 5 Cruise Aircraft Using Endothermic Fuel,", "journal": "Journal of Aircraft,year: 1992", "abstract": null, "author-in-focus": "S C Jones", "cluster_name": "SCJones2", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9755763, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["A Blum", "P Heidelberger", "S S Lavenberg", "M K Nakayama", "P Shahabuddin"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["A Blum", "P Heidelberger", "S S Lavenberg", "M K Nakayama", "P Shahabuddin"]}, "institution": null, "title": "System Availability Estimator (SAVE) Language Reference and User's Manual Version 4.0", "journal": "Research Report 219year: 1993", "abstract": null, "author-in-focus": "A Blum", "cluster_name": "AlvinBlum", "altTitle": null, "editor": "S, IBM T.J. Watson Research Center,", "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["A Blum", "V Kumar", "A Rudra", "F Wu"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["A Blum", "V Kumar", "A Rudra", "F Wu"]}, "institution": null, "title": "Online learning in online auctions", "journal": "in Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (year: 2003", "abstract": null, "author-in-focus": "A Blum", "cluster_name": "AvrimBlum", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D Allen", "D E Tyler"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D Allen", "D E Tyler"]}, "institution": null, "title": "Pathophysiology of acute abdominal disease", "journal": null, "abstract": null, "author-in-focus": "D Allen", "cluster_name": "DAllen-jr", "altTitle": "In: The Equine Acute Abdomen edn", "editor": "., Ed: N.A. White. Lea and Febiger,", "year": null, "keyword": null, "author-in-focus-score": 0.8168709, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D Allen", "A Darwiche"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D Allen", "A Darwiche"]}, "institution": null, "title": "New advances in inference by recursive conditioning", "journal": "In Proceedings of the 19th Conference on uncertainty in Artificial Intelligence,year: 2003", "abstract": null, "author-in-focus": "D Allen", "cluster_name": "DAllen-ucla", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.8168709, "email": null}]}], "match": [{"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["P J Russell", "J M Doenias", "S J Russell"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["P J Russell", "J M Doenias", "S J Russell"]}, "institution": null, "title": "GELYMAC: a Macintosh application for calculating DNA fragment size from gel electrophoresis migration data", "journal": null, "abstract": null, "author-in-focus": "S J Russell", "cluster_name": "SarahJRussell", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.60451025, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["SJ Russell", "KA Steger", "SA Johnston"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["SJ Russell", "KA Steger", "SA Johnston"]}, "institution": null, "title": "Subcellular localization, stoichiometry, and protein levels of 26 S proteasome subunits in yeast", "journal": "J Biol Chemyear: 1999", "abstract": null, "author-in-focus": "SJ Russell", "cluster_name": "SarahJRussell", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.6251166, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D M Allen", "K J Tarnowski"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D M Allen", "K J Tarnowski"]}, "institution": null, "title": "Depressive characteristics of physically abused children", "journal": "Journal of Abnormal Child Psychology,year: 1989", "abstract": null, "author-in-focus": "D M Allen", "cluster_name": "DMAllen-ohu", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.79132205, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["J E Grizzle", "D M Allen"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["J E Grizzle", "D M Allen"]}, "institution": null, "title": "Analysis of growth and dose response curves,", "journal": "biometricsyear: 1969", "abstract": null, "author-in-focus": "D M Allen", "cluster_name": "DMAllen-ohu", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.79132205, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Alvin M Blum"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Alvin M Blum"]}, "institution": null, "title": "A General Purpose Digital Simulator and Examples of Its Application Part III: Digital Simulation of Urban Traffic", "journal": null, "abstract": null, "author-in-focus": "Alvin M Blum", "cluster_name": "AlvinBlum", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9832195, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Alvin M Blum", "Ambuj Goyal", "Philip Heidelberger", "Stephen S Lavenberg", "Marvin K Nakayama", "Perwez Shahabuddin"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Alvin M Blum", "Ambuj Goyal", "Philip Heidelberger", "Stephen S Lavenberg", "Marvin K Nakayama", "Perwez Shahabuddin"]}, "institution": null, "title": "Modeling and Analysis of System Dependability Using the System Availability Estimator", "journal": null, "abstract": null, "author-in-focus": "Alvin M Blum", "cluster_name": "AlvinBlum", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9832195, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["S Jones"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["S Jones"]}, "institution": null, "title": "Graphical interfaces for knowledge engineering: An overview of relevant literature", "journal": "Knowledge Engineering Review,year: 1988", "abstract": null, "author-in-focus": "S Jones", "cluster_name": "SJonesKnowEng", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["S Jones"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["S Jones"]}, "institution": null, "title": "3-D Diagrams for Knowledge Engineering: An Early Estimation of Utility", "journal": null, "abstract": null, "author-in-focus": "S Jones", "cluster_name": "SJonesKnowEng", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["M A Jordan", "K Wendell", "S Gardiner", "W B Derry", "H Copp", "L Wilson"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["M A Jordan", "K Wendell", "S Gardiner", "W B Derry", "H Copp", "L Wilson"]}, "institution": null, "title": "Mitotic block induced in HeLa cells by low concentrations of paclitaxel (taxol) results in abnormal mitotic exit and apoptotic cell death", "journal": "Cancer Res.year: 1996", "abstract": null, "author-in-focus": "M A Jordan", "cluster_name": "MAJordan", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9277701, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["W B Derry", "L Wilson", "M A Jordan"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["W B Derry", "L Wilson", "M A Jordan"]}, "institution": null, "title": "Substoichiometric binding of taxol suppresses microtubule dynamics", "journal": "Biochemistry.year: 1995", "abstract": null, "author-in-focus": "M A Jordan", "cluster_name": "MAJordan", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9277701, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["E Segal", "D Koller"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["E Segal", "D Koller"]}, "institution": null, "title": "lymphomagenesis in the bursa of fabricius", "journal": "PNAS,journal: Inyear: 2001", "abstract": null, "author-in-focus": "D Koller", "cluster_name": "DKoller", "altTitle": "Probabilistic hierarchical clustering for biological data", "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D Bowman", "D Koller", "L F Hodges"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D Bowman", "D Koller", "L F Hodges"]}, "institution": null, "title": "A methodology for the evaluation of travel techniques for immersive virtual environments", "journal": "Virtual Reality: Journal of the Virtual Reality Society,year: 1998", "abstract": null, "author-in-focus": "D Koller", "cluster_name": "DKoller", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["L H Lee", "K Poolla"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["L H Lee", "K Poolla"]}, "institution": null, "title": "Statistical validation for uncertainty models", "journal": null, "abstract": null, "author-in-focus": "L H Lee", "cluster_name": "LHLee-elec", "altTitle": "Feedback Control, Nonlinear Systems, and Complexity,", "editor": "In B. Francis and A.R. Tannenbaum, editors,", "year": null, "keyword": null, "author-in-focus-score": 0.7979153, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["A Jeffery", "L H Lee", "J Q Shields"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["A Jeffery", "L H Lee", "J Q Shields"]}, "institution": null, "title": "Model tests to investigate the effects of geometrical imperfections on the NIST calculable capacitor,", "journal": "IEEE Trans. Instrum. Meas.year: 1999", "abstract": null, "author-in-focus": "L H Lee", "cluster_name": "LHLee-elec", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.7979153, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["D Kuokka", "J McGuire", "J Weber", "J Tenenbaum", "T Gruber", "G Olson"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["D Kuokka", "J McGuire", "J Weber", "J Tenenbaum", "T Gruber", "G Olson"]}, "institution": null, "title": "SHADE:", "journal": null, "abstract": null, "author-in-focus": "J McGuire", "cluster_name": "JGMcGuire", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["T Finin", "J Weber", "G Wiederhold", "M Genesereth", "R Fritz-son", "D McKay", "J McGuire", "R Pelavin", "S Shapiro", "C Beck"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["T Finin"]}, "keyword": null, "title": "Specification of the KQML agent-communication language", "journal": "Technical Report EIT TR 92-04,year: 1992", "abstract": null, "author-in-focus": "J McGuire", "cluster_name": "JGMcGuire", "altTitle": null, "email": null, "editor": null, "year": null, "author-in-focus-score": 1.0, "institution": "DARPA Knowledge Sharing Initiative, External Working Group,"}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Alan Moore"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Alan Moore"]}, "institution": null, "title": "Extending the uml rt profile to support the osek infrastructure", "journal": "In Proceedings of Fifth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing,year: 2002", "abstract": null, "author-in-focus": "Alan Moore", "cluster_name": "AlanMoore", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["M J McLaughlin", "A Moore"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["M J McLaughlin", "A Moore"]}, "institution": null, "title": "Real-time extensions to UML", "journal": "Dr. Dobb's Journal,year: 1998", "abstract": null, "author-in-focus": "A Moore", "cluster_name": "AlanMoore", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Toms Feder", "Rajeev Motwani", "Liadan O'Callaghan", "Chris Olston", "Rina Panigrahy"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Toms Feder", "Rajeev Motwani", "Liadan O'Callaghan", "Chris Olston", "Rina Panigrahy"]}, "institution": null, "title": "Computing Shortest Paths with Uncertainty", "journal": null, "abstract": null, "author-in-focus": "Rajeev Motwani", "cluster_name": "RajeevMotwani", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}, {"body": "1. Introduction Finding implication and similarity rules are two of the most interesting issues in the data mining area. Finding implication rules, also known as association-rule mining, was initially proposed by Agrawal, Imielinski, and Swami [1]. Finding similarity rules is also useful for various kinds of data-mining such as copy detection, clustering and collaborative filtering [5, 16, 9, 12, 11, 17]. Suppose that we have a set of transaction data D that has n transactions and m boolean attributes: A 1 ; A 2 ; : : : ; Am . We say A i ) A j is an implication rule if the fraction of the transactions that contain both A i and A j among those transactions that contains A i is more than a minimum confidence. \\Lambda Hitachi Ltd., Central Research Laboratory. This work was done while the author was on leave at the Department of Computer Science, Stanford University. y Department of Computer Science, Stanford University. Supported in part by NSF Grant IIS-9811904. z Department of Computer Science, Stanford University. Supported in part by NSF Grant IIS-9811904. We also say A i ' A j is a similarity rule if the fraction of the transactions that contain both A i and A j among those transactions that contains either A i or A j is more than a minimum similarity. The goal is to identify all valid rules for a given data. Most of algorithms proposed in the past few years are based on support pruning, which prunes the attributes that have low frequency [1, 2, 4]. This approach extracts association rules with high support (i.e., high frequency) efficiently. However they discard low-support items all the time. Example 1.1 : Suppose that we want to extract similar pages in the Web by analyzing the page-link graph. In order to extract pages that have similar sets of page links, we transform a page-link graph to a binary matrix whose columns represent source pages and whose rows represent destination pages. If a page p i has a link to p j , then the column c i for the row r j is set to 1. Using a high-support threshold, we can only get similarity rules between pages that have many links, such as a directory page. We have focused on finding high-confidence implication and similarity rules without support pruning. We have done some work on this issue and earlier developed a family of algorithms such as Min-Hash and Locality-Sensitive hashing schemes [7, 8, 10]. These algorithms use randomized techniques [13], and they can extract similar pairs very efficiently. However, these algorithms still have a chance of yielding false positives and false negatives. In this paper we propose a family called Dynamic Miss-Counting (DMC) algorithms to avoid both false negatives and false positives. Let M be a boolean matrix that represents the data D. M has n rows and m columns. Each row represents a transaction in D, and in each row, a column c i is set to '1' if the transaction has an attribute A i . The DMC algorithm uses a confidence pruning technique, rather than support pruning. The idea of pruning for high-confidence pairs is not new. Sergey Brin did some (unpublished) experiments, and the paper [3] likewise explored iterative methods for converging on the pairs of columns with highest correlation. These methods never look at all pairs of columns, and make many passes over the data. They are very expensive techniques useful for enormous data sets (e.g., Brin used them to look for correlations among the 100 million or so words that appear on the Web). Our methods are useful for sets with somewhat smaller numbers of columns, they extract all pairs of columns with a similarity or implication that is above a small threshold, and they use only two passes through the data and realistic amounts of main memory. The key idea of the DMC algorithm is counting the numbers of rows in which each pair of columns disagree instead of counting the number of hits, and deleting a counter as soon as the number of misses exceeds the maximum number of misses allowed for that pair. T 1 : A 2 , A 3 T 2 : A 1 , A 2 , A 3 T 3 : A 1 T 4 : A 1 , A 2 T 5 : A 2 , A 3 (a) Data D (b) Matrix M 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 r 1 r 2 r 3 r 4 r 5 c 1 c 2 c 3 Figure 1. Data format Example 1.2 : Fig. 1 is an example of D and M . Suppose that we would like to extract 100%-confidence implication rules for this matrix M . In this sample case, no miss at all between two columns is allowed. When we read r 1 , we have to keep two candidates: c 2 ) c 3 and c 3 ) c 2 . Next, when we read r 2 , we only have to add two more candidates: c 1 ) c 2 and c 1 ) c 3 . We do not have to add candidates such as c 2 ) c 1 or c 3 ) c 1 , since they have already had 1 miss at r 1 . To detect the number of misses from one column to another that has already occurred, we maintain a counter for each column giving the number of 1's in that column seen so far. Since the c 2 counter is 1 at r 2 , we find that the candidate pairs that are not in the candidate list for c 2 have already had 1 miss, which is too many in this simple example. Furthermore, when we read r 3 , we can immediately delete candidates c 1 ) c 2 and c 1 ) c 3 , since these candidates miss at this row. After reading all rows in the matrix M , only one rule, c 3 ) c 2 , survives. Example 1.3 : Consider a column c i that has 100 1's, and suppose we want to find implication rules with 85% or more confidence. In this case, the number of misses from c i to any other column must not be more than 15. Therefore, we can delete a counter for candidate pairs c i ) c j as soon as the number of misses exceeds 15. Furthermore, we do not have to add a new counter for c i after we have seen 16 rows in which c i is set to 1, because a column c j that has not yet appeared already has had 16 misses for the rule c i ) c j . Note that this algorithm requires as many as m 2 counters in the worst case. However, in real data such as Web-page-link graphs, most pages are linked to ten or so pages, while the number of pages is in the millions. Therefore the number of counters for most pages will not approach, even remotely, the number of pages. This fact significantly reduces the memory requirements and computation cost. In this paper we also propose several techniques such as row re-ordering, memory-explosion elimination, 100%rule pruning, column-density pruning and maximum-hits pruning, each of which contribute to reducing the size of memory significantly. We start to define our problem in Section 2. In Section 3 we present conventional data miningalgorithms and our new algorithm. We have also applied many other optimization techniques to our DMC algorithm, which we mention in Section 4. We then present a variant algorithm for finding similarity rules in Section 5. In Section 6 we describe the data that we used to evaluate algorithms and show the experimental results of algorithms. We conclude in Section 7 by discussing some extensions of our work to apply our approach to extract more complicated rules among three or more attributes. 2 Problem statement We view the data as a 0=1 matrix M with n rows and m columns. Each column represents an \"attribute,\" and each row represents a \"transaction.\" Define S i to be the set of rows that have a 1 in column c i . We define the confidence of the rule c i ) c j as Conf(c i ; c j ) = jS i `` S j j jS i j That is, the confidence of c i and c j is the fraction of rows, among those containing a 1 in c i , that contain a 1 in both c i and c j . Note that if jS i j ! jS j j, then Conf(c j ; c i ) ! Conf(c i ; c j ). Therefore, we consider only rules c i ) c j such that jS i j ! jS j j or (jS i j = jS j j and i ! j), and our goal is to extract all rules that have minconf or more confidence, where 0 ! minconf 1. We also define the similarity of a column pair, c i ' c j as Sim(c i ; c j ) = jS i `` S j j jS i [ S j j That is, the similarity of c i and c j is the fraction of rows, among those containing a 1 in either c i or c j , that contain a 1 in both c i and c j . Note that this definition is symmetric with respect to c i and c j . Our other goal is to extract all combinations of column pairs that have minsim or more similarity, where 0 ! minsim 1. 3 Algorithms In this section we review conventional algorithms: a-priori and Min-Hash. Then we overview our new Dynamic Miss-Counting algorithm. 3.1 A-priori algorithm A-priori [1, 2], which uses a support pruning technique to reduce the search space, is the most famous and one of the most effective algorithms for finding association rules. It prunes the candidate pairs if the frequency of each column by itself does not exceed the minimum support threshold. However, there are some cases where support pruning does not work very well. For example, consider the data in Fig. 1, with minimum support, minsup, of 50% and minimum confidence, minconf, of 85%. Since all columns have 3 or more 1's in M , no candidate pairs can be pruned by a-priori, and it requires as much memory as m(m \\Gamma 1)=2 counters. To reduce the number of counters, the DHP [14] algorithm, which uses a hash-based technique to prune candidate pairs, was proposed. This algorithm works well to prune most of the useless counters in some cases, while it does not solve the problem in the next paragraph. The most significant problem for these algorithms is that if many columns in the matrix remain after support pruning, they must use too many counters in main memory. For example, our Web-page-link data has about 700,000 columns, and even if we prune the pages that have less than 10 1's, there still remains 58,000 columns. Therefore about 1.7 billion counters would have to fit in main memory. 3.2 Min-Hash algorithm [7, 8] proposed the Min-hash algorithm in order to find all similar pairs without support pruning. The basic idea in the Min-hash algorithm is to permute the rows randomly, and for each column c i , to compute its hash value h(c i ) to be the index of the first row under the permutation that has a 1 in that column. To avoid physically permuting rows, we instead give a random hash number to each row, and extract for each column the minimum hash value of any row where the column has a 1. Since for any column pair (c i ; c j ), P rob[h(c i ) = h(c j )] = Sim(c i ; c j ), we can estimate similarity by repeating the random min-hashing process k times. Note that we can generate all k min-hash values with a single data scan, as explained in [8]. This algorithm works very well to find almost all truly similar pairs. However, it can not find all pairs with 100% reliability, because there is a small probability of generating false negatives. Furthermore, generated candidates have to be verified to confirm that they actually satisfy the minimum similarity. 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 r 1 r 2 r 3 r 4 r 5 r 6 r 7 r 8 r 9 c 1 c 2 c 3 c 4 c 5 c 6 (a) Data matrix 1 c 6 /0 2 c 4 /1 c 5 /0 1 2 0 c 1 c 2 c 3 c 4 c 5 1 c 6 c 5 /0 cnt List of (id, miss) (b) Counter array before r 4 2 c 6 /0 3 c 5 /1 1 2 1 c 1 c 2 c 3 c 4 c 5 2 c 6 c 5 /0 c 2 /0 c 3 /0 c 6 /0 c 3 /1 (c) Counter array after r 4 5 5 c 5 /1 5 5 5 c 1 c 2 c 3 c 4 c 5 5 c 6 c 2 /1 (d) Final result Figure 2. DMC Algorithm 3.3 Dynamic Miss-Counting algorithm Before describing the DMC algorithm in detail, let us introduce an example that shows how our algorithm works. Example 3.1 : Fig. 2 is example data that has 9 rows. suppose that we want to extract 80%-or-more-confidence implication rules. Since each column contains five 1's, only one miss per column is allowed. Fig. 2(b) shows the data structure that: 1. Counts number of 1's that have already appeared in each column. 2. Keeps lists of candidates and miss counters for these candidates. For instance, before processing r 4 , 4 candidate rules are kept in the lists: c 2 ) c 6 , c 3 ) c 4 , c 3 ) c 5 , and c 4 ) c 5 . Recall that we only keep candidates such that jS i j ! jS j j or (jS i j = jS j j and i ! j). We do not have to keep an entry for c 6 , because c 6 has the minimum number or 1's and the largest column ID number. Since c 3 ) c 4 has already missed at r 3 , its miss counter is equal to 1, other miss counters are 0. At r 4 , we process as follows: ffl c 1 : Since c 1 appears for the first time, every column in r 4 is set as a candidate with c 1 . In this example, c 2 , c 3 , and c 6 are placed in the list for c 1 . ffl c 2 : Though c 2 has already appeared in r 1 , the c 2 counter is equal to or less than the maximum number of misses allowed, which is 1 in this example. therefore we add to the list for c 2 every column in r 4 that has not previously appeared on the list for c 2 , which is only c 3 in this case. Note that the new candidates have already missed as many times as the c 2 counter's value, which is 1 here. A candidate that has already been added to c 2 's list, but that does not appear in this row, should have its miss counter increased (there is no such a column in this example). ffl c 3 : Since c 3 has already appeared in r 2 and r 3 , a column that has not yet been added to the candidate list for c 3 has had 2 misses already. Therefore, we do not have to add new candidates for c 3 . We only have to check the candidates in its candidate list, and delete a candidate whose miss counter exceeds the maximum misses. In this example, both c 4 and c 5 get misses, and only c 5 survives. Fig. 2(c) shows the counter array after processing r 4 . Fig. 2(d) is the final result after reading all rows, which shows c 1 ) c 2 and c 3 ) c 5 have at least 80% confidence. Algorithm 3.1 : DMC-base 1. Read M and count the number of 1's for each column, ones(c i ). 2. Calculate the maximum number of misses for each column, maxmis(c i ) = b(1 \\Gamma minconf ) \\Theta ones(c i )c. Clear the counter and candidate list for each column, i.e., cnt(c i ) = 0 and cand(c i ) = NULL. 3. In the second scan, for each row r i of M do: (a) For each column c j that has 1 in a row r i (we describe this condition in the form of c j 2 r i , since a row consists of a set of columns), process as following: Case cnt(c j ) = 0: Create a candidate list for c j by adding all columns c k 2 r i such that ones(c k ) ? ones(c j ) or (ones(c k ) = ones(c j ) and k ? j). Set all miss counters for candidates to 0. Case cnt(c j ) maxmis(c j ): Merge the candidate list with the column list in r i . For all columns c k 2 r i [cand(c j ), if c k exists only in r i and c k satisfies the same condition as the above case, add c k into the candidate list, while initializing the miss counter to cnt(c j ). If c k exists only in the candidate list, increase the miss counter for c k . Case cnt(c j ) ? maxmis(c j ): Merge the candidate list for c j with the column c k 2 r i . For all columns c k 2 cand(c j ), if c k does not exist in r i , then increase the miss counter for c k . If the counter exceeds maxmis(c j ), then delete c k from the candidate list for c j . (b) After processing all c j , increase the counter cnt(c j ) for each column c j in r i and continue processing the next row. If cnt(c j ) is equal to ones(c j ), then output all rules c j ) c k , where c k is a column in cand(c j ), and release the candidate list. 4 Memory optimization The DMC-base algorithm reduces the memory requirement significantly, e.g. in the case of Web-page-link data with pruning of columns with fewer than 10 1's, DMC-base requires about 0.33GB memory, while a-priori requires 6.8GB memory. However, our algorithm still requires as much as m 2 main memory in the worst case. In this section we show some techniques that reduce the amount of main memory. 4.1 Row re-ordering Suppose that r 7 is the first row in the matrix M in Fig. 2. We have to create all column pairs as candidates in this case. In general, the denser the rows that come first, the more memory is necessary for the DMC-base algorithm. Therefore we should read sparser rows first in order to reduce the memory size required. In Example 3.1, the history of the total number of candidates in the candidate lists is (1; 4; 4; 7; 9; 7; 7; 6; 2) if we read the matrix M in the original order; while it is (1; 2; 3; 5; 6; 8; 5; 2; 2) if we read M in the sparsest-first order (r 1 ; r 3 ; r 8 ; r 2 ; r 5 ; r 4 ; r 6 ; r 9 ; r 7 ). This technique works very well, especially if the row density of the matrix has a wide distribution. For example, the Web-access log has a few clients such as Web crawlers that access all pages on the site, while most clients access only a few pages. This optimization reduces the memory size significantly, e.g. in the previous case of Web-page-link data with support pruning, we can reduce the memory size from 0.33GB to 0.033GB. However, it is expensive to sort the original data by density. Instead of sorting, we make buckets corresponding to ranges of row density for each row. That is, we divide the original data according to the number of 1's in each row with ranges of [2 i ; 2 i+1 ), when we scan the data the first time. Then, in the next scan, we read the lower density buckets first. Note that the number of buckets is no more than dlog 2 me + 1. (b) Web page-link graph 1 10 100 1000 10000 100000 1000000 0 50000 100000 150000 200000 #of rows processed Memory(KB) 1 10 100 1000 10000 100000 1000000 0 50000 100000 150000 200000 250000 #of rows processed Memory(KB) (a) Web access log Figure 3. Memory size for a counter array 4.2 Memory-explosion elimination Scanning the densest rows last may cause memory explosion at the end of the algorithm. Fig. 3 shows the memory consumption for Web-access-log and Web-page-link data without support pruning, when extracting 100%-confidence rules. The required memory size explodes at the end of the processing, since both data sets have several rows with many 1's. To avoid memory explosion, we switch the algorithm from the memory-consuming algorithm, DMC-base, to an algorithm that uses more time, but uses less space. We assume that there are few rows with many 1's. Therefore, when and if the memory explosion begins, we can read the rest of the rows and create bitmaps for each column in main memory. The low-memory algorithm, DMC-bitmap, consists of 2 phases. In the first phase, it cleans up the candidate list for those c j such that cnt(c j ) ? maxmis(c j ), and in the second phase, it extracts implication rules, c j ) c k , for those c j such that cnt(c j ) maxmis(c j ). Algorithm 4.1 : DMC-bitmap 1. When the memory needed for counters exceeds a threshold, and the bitmaps for the rest of the rows r i (i = t; t + 1; : : : ; n) can fit in main memory, read the rest of the rows r i from M and create (n \\Gamma t + 1) bits of a bitmap, bm(c j ), for each column c j such that cnt(c j ) ! ones(c j ) ---that is, we do not have to create bitmaps for those columns that have no 1's in the rest of rows. In order for this method to work, there must be enough memory to maintain the counter array until such time as the bitmaps will fit in the same memory. 2. (Phase 1) For each column c j that has a non-NULL candidate list and cnt(c j ) ? maxmis(c j ): ffl Count the number of misses for c j ) c k such that c k 2 cand(c j ) by counting the number of 1's in bm(c j ) bm(c k ). ffl If the total number of misses is no more than maxmis(c j ), then output c j ) c k as a result. ffl Free the candidate list for c j . 3. (Phase 2) For each column c j such that cnt(c j ) maxmis(c j ): ffl Initialize hit counter hit(c k ) to 0. If cand(c j ) is non-NULL, set hit(c k ) = cnt(c j ) \\Gamma mis(c j ; c k ) for each column c k 2 cand(c j ), where mis(c j ; c k ) is the number of misses of c j against c k . ffl Count the number of hits in the rest of rows, r i (i = t; t + 1; : : : ; n), by increasing each hit counter hit(c k ) such that c k 2 r i , for each row r i such that r i 2 c j . ffl For each column c k such that ones(c k ) ? ones(c j ) or (ones(c k ) = ones(c j ) and k ? j), if hit(c k ) ones(c j ) \\Gamma maxmis(c j ), then output c j ) c k as a result. 4.3 100%-rule pruning Finding 100%-confidence rules is much easier than finding less-than-100%-confidence rules. We do not have to count the number of misses, since we can delete a candidate as soon as we find a miss for it. Therefore, we only have to keep the candidate ID lists in main memory. Furthermore, we can also simplify both the DMC-base and DMC-bitmap algorithms, since after finding the first 1 on a column c j , there is no chance of adding any candidates into the candidate list, cand(c j ). 1 10 100 1000 10000 100000 1000000 1 10 100 1000 10000 #of 1's in a column #of columns Web access log Page link Dictionary New s Web access log Page link News Dictionary Figure 4. Column density distribution Suppose that we want to extract 90% or more confidence rules. In such a case, a column that has fewer than 9 1's must have no miss. Therefore, if we can extract 100%-confidence rules first, then we only consider those columns that have 10 or more 1's. Fig. 4 shows the distribution of the number of 1's in each column of four real data sets. Since our data has many columns with few 1's, this optimization improves the performance. 4.4 DMC-imp algorithm The final algorithm to extract implication rules is following. Algorithm 4.2 : DMC-imp 1. Read M and count the number of 1's for each column, ones(c i ). Partition the data into buckets B i , i = 0; : : : ; dlog 2 me, according to the number of 1's for each row. 2. Extract 100%-confidence rules by using the simplified DMC-base and DMC-bitmap algorithms. Note we can skip Step 1 of DMC-base and we should scan the sparsest buckets first. In Phase 2 of DMC-bitmap, we count the number of misses between c j and all c k that have at least one of rows r t ; r t+1 ; : : : ; r n in common with c j . 3. Remove columns c j such that ones(c j ) 1=(1 \\Gamma minconf ). 4. Extract less-than-100%-confidence rules by using the DMC-base and DMC-bitmap algorithms. Note that we can skip Step 1 of DMC-base and we should scan the sparsest buckets first. In our implementation, we switch the algorithm from DMC-base to DMC-bitmap when the number of remaining rows becomes 64 or less, and the memory size for the counter array that keeps both miss counters and candidate lists for each column exceeds 50MB. Note that even if the memory size exceeds 50MB, we do not switch the algorithm if the number of remaining rows is more than 64, regardless of the number of columns. 5 Finding similarity rules We can find similarity rules more efficiently than we can implication rules, since we can apply two more optimization techniques: column-density pruning and maximum-hits pruning. 5.1 Column-density pruning If column c i and c j , such that jS i j j, have minsim or more similarity, the ratio of the number of 1's, jS i j=jS j j, must be minsim or more: minsim Sim(c i ; c j ) = jS i `` S j j jS i [ S j i j jS j 1 Therefore we can save the memory space for miss counters for those column pairs c i and c j such that jS i j=jS j j ! minsim, since we do not have to consider such pairs. 5.2 Maximum-hits pruning Similarity is affected by the number of misses of both c i against c j and c j against c i , while confidence depends only on the number of misses of c i against c j , where jS i j j. Therefore, we can prune candidates earlier by considering the maximum number of hits, as follows: 0 1 ... 1 1 ... 0 1 ... 1 1 ... ... ... 1 1 ... 1 0 ... ... ... r 1 r 2 r 3 r 4 ... r x r y ... c 1 c 2 ... 3 c 2 /0 1 c 1 c 2 ... ... Maximum hits = 2 Figure 5. Maximum hits pruning Example 5.1 : Consider a matrix M as Fig. 5. Since we do not count the number of misses between c i and c j such that jS i j ? jS j j, we only maintain a miss counter from c 1 to c 2 . If we want to extract 75%-or-more similar column pairs, one miss is allowed between c 1 and c 2 . At r 2 , we initialize the miss counter for (c 1 ; c 2 ) to 0. Then, at r 4 we check this candidate again. 1 We should delete the miss counter for (c 1 ; c 2 ), even though both c 1 and c 2 are 1 in r 4 . Because cnt(c 1 ) = 1 and cnt(c 2 ) = 3 before reading r 4 , the numbers of remaining 1's for each column are 3 and 2, respectively. Therefore we know that there are at most 2 rows with both c 1 and c 2 in the following rows. Since this pair has already had 1 hit in r 2 , the maximum possible number of hits, c hit, is at most 3, and the maximum possible similarity, d Sim(c 1 ; c 2 ) is c hit=(ones(c 1 ) + ones(c 2 ) \\Gamma c hit) = 0:5. Let rem(c i ) be the number of remaining 1's of c i , i.e. rem(c i ) = ones(c i )\\Gammacnt(c i ). Then, the maximum number of hits with c i and c j such that ones(c i ) ones(c j ) is calculated as follows: ffl If rem(c i ) rem(c j ), all remaining 1's of c i can match 1's of c j . Therefore, the maximum possible hits, c hit, is ones(c i ) \\Gamma mis(c i ) ffl If rem(c i ) ? rem(c j ), then rem(c i ) \\Gamma rem(c j ) of the remaining 1's of c i can not match 1's of c j . Therefore, the maximum possible hits, c hit, is ones(c i ) \\Gamma mis(c i ) \\Gamma (rem(c i ) \\Gamma rem(c j )). Using c hit, we can calculate the maximum possible similarity, d Sim(c i ; c j ) = c hit=(ones(c i ) + ones(c j ) \\Gamma c hit); which can be used for candidate pruning. 5.3 DMC-sim algorithm The overall algorithm to find similar column pairs is following. Algorithm 5.1 : DMC-sim 1. Same as Step 1 of DMC-imp. 2. Extract 100%-similar, i.e. identical, columns. We only have to keep candidates such that ones(c i ) = ones(c j ) with no miss. To avoid memory explosion, we also apply a variant of the DMC-bitmap algorithm. The main points that we should modify it are following: 1 Note that we could have deleted (c 1 ; c 2 ) at r 3 , where c 2 has 1 but c 1 does not. However, in order to apply the maximum-hits pruning from a dense column c d to sparse columns cs (from c 2 to c 1 , in this example), we have to check all candidate lists for sparse columns cs , which increases the computationtime significantly. Therefore we only apply the maximum-hits pruning from a sparse column to dense columns. ffl Compare those columns that have the same number of 1's, since we want to extract identical pairs. ffl Extract those column pairs c i and c j that have the same bitmap instead of counting the number of 1's in bm(c j ) bm(c k ). 3. Remove columns c j such that ones(c j ) 1=(1 \\Gamma minsim ) \\Gamma 1. Note that the cut-off threshold is not 1=(1 \\Gamma minsim ), since there might be less-than-100% similar pairs between the columns whose number of 1's are d1=(1 \\Gamma minsim ) \\Gamma 1e and d1=(1 \\Gamma minsim )e. 4. Extract less-than-100% similar columns by using a variant of DMC-base and DMC-bitmap algorithms. The modification points are following: ffl Skip Step 1 of DMC-base and scan the sparsest bucket first. ffl Keep candidates such that ones(c i )=ones(c j ) is minsim or more. ffl Discard candidates whenever the maximum possible similarity, d Sim(c i ; c j ), is less than minsim. 6 Performance Evaluation We implemented our algorithms, DMC-imp and DMC-sim, on a Sun Ultra 2 (2 \\Theta 200MHz, 256MB memory) workstation. In this section, we describe the data sets that we used for our experiments. Then we show the experimental results of both algorithms. To compare DMC algorithms with a-priori and Min-Hash algorithms, we have done an experiment with applying both support pruning and confidence pruning. Note that support pruning can be applied to DMC and Min-Hash algorithms in the same manner as a-priori. Finally, we show sample rules extracted from news articles. 6.1 Data sets Table 1 shows the size of the data sets we used in our experiments. The meaning of each data set is followings: 1. Web access log: This data set consists of the log entries from the sun Web server. The columns in this case are the URL's and the rows represent distinct client IP address that have accessed the server recently. An entry is set to 1 if there has been at least one hit for that URL from that particular client IP. The data set, Wlog, has more than 200,000 rows and 75,000 columns. We also prepared the data set WlogP by pruning those columns with 10-or-fewer 1's. The pruned data set WlogP has 13,000 columns. 2. Web-page-link graph: This data represents the URL link graph for the Stanford site, which has about 700,000 pages. Both columns and rows are the URL's. An entry is set to 1 if there is a link from the page p i to the page p j . The rows are p i and the columns are p j in the data plinkF, and the rows are p j and the columns are p i in the data plinkT. Extracting similar columns from plinkF means extracting pages that are referred to by similar sets of pages, while extracting from plinkT means extracting pages that have similar sets of links. 3. News documents: This data, News, consists of 84,000 Reuters news documents. Each row is a document, and each column is a word. Stop words, which appear among the documents very frequently, are pruned before processing, and 170,000 words remain. By using implication rules with low-support pruning, we can get sets of words each of which is related to a news topic. To compare the performance with other algorithms, we also prepare a smaller data set, NewsP, since other algorithms could not be applied to our data sets in a reasonable execution time on our machine due to its memory size. 4. Dictionary: This is an on-line version of the 1913Web-ster's dictionary that is available throughthe Gutenberg Project [15]. Columns are head words (words being defined) and rows are definition words (words used in the definition). There are 96,000 head words and 45,000 definition words. We can get similar definition words, such as 'brother-in-law' and 'sister-in-law', by extracting similarity rules. Table 1. Real data sets Data Rows Columns Wlog 218,518 74,957 WlogP 203,185 13,087 pliknF 173,338 697,824 plinkT 695,280 688,747 News 84,672 170,372 NewsP 16,392 9,518 dicD 45,418 96,540 6.2 Experimental results In this section we discuss our experimental results. We use 6 sets of the data -- Wlog, WlogP, linkF, linkT, News, and dicD -- for evaluating DMC-imp and DMC-sim. We also use one small data set NewsP for comparing the performance among the algorithms including a-priori and Min-hash. Fig. 6 shows the experimental results. The left graphs are for finding implication rules, and the right graphs are for finding similarity rules. Fig. 6(a) and (b) plot the execution time with a different threshold. Each execution can be finished in a reasonable time if we want to extract 85%-or-more rules. The execution time decreases lineally according to increasing the threshold, in general. Fig. 6(c) and (d) shows the details of the execution time for Wlog. The execution time for pre-scanning is small compared to other execution times. The execution time for finding 100% rules is also small and constant for each threshold, while the execution time for finding less than 100% rules depends on the threshold. The execution times of DMC-imp and DMC-sim for plinkT have a gap between 80% and 75% threshold. Fig. 6(e) and (f) show the detail of the execution time. In these cases, the low-memory algorithm, DMC-bitmap, jumps up from 22 to 398 seconds in the case of DMC-imp and from 27 to 399 seconds in the case of DMC-sim, respectively. The reason why this jump-up occurs is that larger rows that are handled in the DMC-bitmap includes many columns that have frequency 4. Since we can not prune columns with frequency 4 if the threshold is 75% or less, the execution time for DMC-bitmap was dramatically increased. If we could apply low-support pruning before executing algorithms, it would not occur. Fig 6(g) and (h) show the maximum memory size for the counter array that keeps candidate IDs and their miss-counters. DMC-sim requires much less memory than DMC-imp, since DMC-sim can use the two additional pruning techniques that we mentioned in Sec. 5. Except DMC-imp for News with 75%-or-less threshold, all executions can fit in main memory. Since we apply the DMC-bitmap algorithms, the memory requirement does not explode even as the threshold decreases. For example, the memory requirement for plinkT does not jump up even if the threshold decreases from 80% down to 70%, while the execution time does. In order to compare the DMC algorithms to a-priori and Min-Hash, we generated a pruned data set of news documents, NewsP. We created this data from 16,392 news documents, and pruned the columns with minimum support threshold 35 (0.2%) and maximum support threshold 3278 (20%). Since the number of remaining columns was 9518, the counters for all pairs could be fit in mainmemory (that required 172MB). This situation is best for a-priori, since the memory optimization techniques for DMC and Min-Hash will not improve their performance significantly. Fig 6(i) and (j) show the execution times of these algorithms. The K-Min algorithm is a variant algorithm of Min-Hash, which can extract implication rules instead of similarity rules. However, it could not extract complete sets of true rules; therefore we plotted the execution time when the number of false negatives was less than 10%. The other algorithms, including Min-Hash for finding similarity rules, 0 100 200 300 400 500 600 700 800 65 70 75 80 85 90 95 100 Confidence threshold(%) Execution time (sec) 0 100 200 300 400 500 600 700 800 65 70 75 80 85 90 95 100 Similarity threshold(%) Execution time (sec) Wlog WlogP plinkF plinkT News dicD 0 50 100 150 200 65 70 75 80 85 90 95 100 Confidence Threshold (%) Memory (MB) 0 50 100 150 200 65 70 75 80 85 90 95 100 Similarity Threshold (%) Memory (MB) Wlog WlogP plinkF plinkT News dicD (a) Execution Time (b) Execution Time (g) Max. memory size (h) Max. memory size (e) DMC-imp (plinkT) 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Confidence threshold (%) Time (sec) (c) DMC-imp (Wlog) 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Confidence threshold (%) Time (sec) (f) DMC-sim (plinkT) 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Similarity threshold (%) Time (sec) (d) DMC-sim (Wlog) 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Similarity threshold (%) Time (sec) <100-bitmap <100%-base 100%-bitmap 100%-base Pre-scaning (i) Implication algorithms (NewsP) 0 50 100 150 200 65 70 75 80 85 90 95 100 Confidence threshold (%) Time (sec) DMC-imp a-priori K-Min (j) Similarity algorithms (NewsP) 0 50 100 150 200 65 70 75 80 85 90 95 100 Similarity threshold (%) Time (sec) DMC-sim a-priori Min-Hash Figure 6. Experimental results polgar -} international polgar -} old polgar -} judit polgar -} champion polgar -} youngest polgar -} chess polgar -} kasparov polgar -} men polgar -} highest polgar -} top polgar -} soviet polgar -} players polgar -} federation polgar -} player polgar -} ranked polgar -} grandmaster polgar -} garri judit -} soviet judit -} hungary kasparov -} chess kasparov -} game kasparov -} champion grandmaster -} soviet grandmaster -} champion grandmaster -} chess garri -} chess garri -} kasparov garri -} soviet garri -} championship garri -} champion Figure 7. Sample rules could extract complete sets of true rules. In this experiment, a-priori is best for finding implication rules with 75%-or-less confidence threshold, and Min-Hash is best for finding similarity rules with70%-or-less similarity threshold, respectively. However, the DMC algorithms are best for finding both implication and similarity rules with high threshold. 6.3 Extracted rules Text-mining by using extracted implication rules with low-supportpruning is one of the interesting applications for our algorithms. Fig. 7 shows sample rules we extracted from News with an 85% confidence threshold and with a support pruning less than 5. These rules are extracted by selecting all rules related to keyword Polgar and its successors, recursively. This set of rules indicates information about Miss Judit Polgar, who is 12-years-old, has been ranked No. 1 in the women's world chess. 7 Conclusion and future works We presented two new algorithms, DMC-imp and DMC-sim, for finding implication and similarity rules. Our algorithms do not use support pruning but use confidence or similarity pruning, which reduces the memory size significantly. We also proposed the other pruning techniques, row re-ordering, 100%-rule pruning, column-density pruning, and maximum-hits pruning. In order to evaluate the performance of the algorithms, we used 4 sets of data, Web-access logs, Web-page-link graph, news documents, and a dictionary. The algorithms have been implemented on a Sun Ultra 2 (2 \\Theta 200MHz, 256MB memory) workstation. According to the experimental results, our algorithms can be executed in a reasonable time. The algorithms that proposed previously can not execute on our data sets, since the algorithms required more than 256MB memory. Therefore, we compared performance by using the News data sets by pruning by using support threshold 35 so that all counters for a-priori can fit in main memory. The comparison results shows that DMC-imp can execute 1.7 times faster than a-priori, and 1.9 times faster than K-Min, and that DMC-sim can execute 5.9 times faster than a-priori, and 1.7 times faster than Min-Hash, in case of an 85% threshold. The followings are future research topics: ffl Our algorithm can not extract rules among more than two columns, while a-priori can do so. However, by grouping similarity and implication rules as showed in Sec. 6.3, we can get useful groups of rules among more than two columns. This idea can be applied to other data sets, which generates more interesting rules. ffl The memory requirement for News with less than 80% confidence threshold exceeds 256MB. To be scalable this algorithm, a parallel algorithm based on a divide-and-conquer technique, such as FDM [6] for a-priori, is necessary.", "authorlist": {"__class__": "tuple", "__value__": ["Shinji Fujiwara", "Jeffrey D Ullman", "Rajeev Motwani"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Shinji Fujiwara", "Jeffrey D Ullman", "Rajeev Motwani"]}, "institution": null, "title": "Dynamic Miss-Counting Algorithms: Finding Implication and Similarity Rules with Confidence Pruning", "journal": null, "abstract": "Abstract Dynamic Miss-Countingalgorithms are proposed, which find all implication and similarity rules with confidence pruning but without support pruning. To handle data sets with a large number of columns, we propose dynamic pruning techniques that can be applied during data scanning. DMC counts the numbers of rows in which each pair of columns disagree instead of counting the number of hits. DMC deletes a candidate as soon as the number of misses exceeds the maximum number of misses allowed for that pair. We also propose several optimization techniques that reduce the required memory size significantly. We evaluated our algorithms by using 4 data sets, i.e., Web access logs, Web page-link graph, News documents, and a Dictionary. These data sets have between 74,000 and 700,000 items. Experiments show that DMC can find high-confidence rules for such a large data sets efficiently.", "author-in-focus": "Rajeev Motwani", "cluster_name": "RajeevMotwani", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": "1 Introduction Mobile robots are being used increasingly in safety-critical applications, such as waste cleanup, space, and the military. Due to the sensitive nature of such domains, human guidance is important to ensure that the robots are attending to the right set of goals. On the other hand, humans may not want, or be able, to exercise detailed control over the robots. This is especially true in situations where multiple robots need to be controlled and in situations, such as military operations, where the human operators themselves are under stress. In such situations, robots must be highly flexible, autonomous assets that can carry out complex tasks with only minimal command effort from humans. This paper describes an implemented, integrated system that enables a single user to easily control and coordinate multiple robots. The architecture (Figure 1) follows the now-common tiered approach to autonomous systems (cf. 3T [4]). In particular, in our architecture the top layer is a task planner, connected to a graphical user interface (GUI), that supports a \"playbook\" style control strategy, the middle layer is a task-level executive that flexibly coordinates heterogeneous robots, and the bottom layer, replicated on each robot, performs reliable autonomous navigation using probabilistic representations. This paper describes how the system is used to coordinate the deployment of heterogeneous robots into a previously-mapped area. In [5] and [19], we describe how the same basic system is used to coordinate multiple robots in exploring and mapping previously unknown areas. This work is being carried out under the Tactical Mobile Robot (TMR) program, sponsored by DARPA. The goal of the TMR program is to enable war fighters to easily deploy and control teams of small, portable robots in urban settings. Such robots can be used to explore buildings, guard locations, establish communication networks, provide distractions, assist the injured, etc. To avoid interfering with normal military operations, the robots must be fairly autonomous, reliable, and easy to control. Key aspects of the work presented here include: . The graphical user interface is designed around the notion of a \"playbook\", in which a user commands the robots using a small set of intuitive, parameterized strategies. This project demonstrated the playbook concept and validated the ability to rapidly task teams of robots with minimal user input. . The MACBeth planner accurately decomposes high-level user intentions into executable robot executive programs. MACBeth is able to communicate tasks to the executive at multiple levels of abstraction. . The executive uses a high-level task description language to represent the coordination strategies of the robots. In particular, we show how the language enables qualitatively different methods of deployment to be specified with only minor changes in syntax. . The navigation system uses probabilistic representations to reliably track and guide the robots. The same system works on several different platforms, with different sensor and actuator configurations. Figure 1: Tiered Architecture for Autonomous Robots Navigation/ Behavioral Navigation/ Behavioral Navigation/ Behavioral Executive Planner/ GUI The next section describes in more detail the scenario that motivates our work. Section 3 presents related work in multi-robot deployment and coordination. Sections 4, 5 and 6 present the planner/GUI, executive, and navigation layer, respectively. Section 7 describes some of our experience with the multi-robot deployment system, and Section 8 offers conclusions. 2 Scenario The scenario here is the coordinated deployment of robots in a previously-mapped environment. For example, several robots might have been tasked with exploring an area to make a map [19] and then return to \"base\". After that, a larger team of robots might be deployed to strategic positions in the building, for instance to provide line-of-sight communications, or to monitor for intruders. We want users to have to specify only the deployment locations and a general strategy for the deployment, and to have the software system determine which robots go where (based on their different capabilities) and how to navigate to the locations in a coordinated way. The deployment strategies differ in the order and degree of concurrency used in moving the robots. Each deployment strategy is a type of set \"play\", parameterized by the number and end locations of each robot. We have investigated three different deployment strategies. The group deployment has all the robots move concurrently to the closest chosen location, then one stays behind and the rest move to the second closest location, etc., until the last robot travels (alone) to the last location. In contrast, the wave deployment strategy uses the notion of a \"point man\" -- one robot goes to the first location and, when it arrives, the second robot travels to the first position while the first (\"point man\") robot moves on to the second location. The robots continue to deploy in a wave-like manner until the first robot reaches the final location, at which point the last robot has also arrived at the first location. In the leap-frog deployment, a \"point man\" robot travels to the first location, then a second robot travels past the first robot to the second location, with each subsequent robot traveling past all the previously deployed robots to get to the next location. Issues in this scenario include planning out which robots should go where, coordinating their interactions to implement the desired deployment strategies, getting the robots to navigate reliably, and minimizing human involvement in all this. After presenting related work, we describe our approaches to these problems. 3 Related Work While there has been much prior work in multi-robot cooperation, we focus here on a few relevant pieces of work. Several authors have investigated multi-robot formations, mostly concentrating on the behavioral aspects of maintaining formation, rather than the sequential aspects of deployment that is the focus of this paper. Balch and Arkin [2] developed behaviors to maintain different types of formations (line, wedge, etc.) under different conditions (leader-centered, unit-centered, etc.). They showed how the behaviors were able to reliably maintain formation and smoothly switch between formations. While they did not address planning or explicit robot synchronization, their methods could be combined with our own, at the behavioral level. Other authors [6, 21] have looked at formation maintenance in a control-theoretic framework, especially with regards to the stability of formations in situations with only limited inter-robot communication. Parker [15], in her ALLIANCE architecture, is also able to handle formation maintenance, among other multi-robot tasks. While still not employing explicit coordination, the robots use the concepts of \"motivation\" and \"impatience\" to effectively cooperate, and to do so in a fault-tolerant manner. Jennings et. al. [12] have developed a distributed executive for multi-robot coordination. The executive, based on a distributed dialect of Scheme, is similar to our executive language in the types of explicit synchronization constraints it supports. One difference with our work is that Jennings' executive is not integrated with a planner. Another difference is that his executive is truly distributed amongst the robots. This is something that we are currently implementing for our executive language. Alami et. al. [1] present a general methodology for distributed planning and plan execution. This contrasts with our approach in which the planning is centralized. They describe methods for incrementally merging plans in a distributed fashion. Like our (and Jennings) work, temporal constraints are used to coordinate activities, and these constraints enable tasks to be invoked automatically on one robot when another robot finishes a task. An interesting aspect of the work is that the plan merging activities may themselves be scheduled and coordinated. 4 The Playbook GUI and MACBeth Planner In our work, multi-robot deployment is commanded using a prototype playbook GUI [13], which provides a high-level command and control interface requiring minimal human attention. The concept is derived from sports playbooks, where each player knows the broad outline of a \"play,\" and each play has been well-practiced by the team. During an actual game, the play is \"tailored\" by a coach or team captain to meet the conditions on the field. Similarly, the playbook interface provides easy access to a set of predefined mission profiles (deployment maneuvers), along with tailoring functions that allow the user (a soldier) to quickly and easily customize the deployment for a particular scenario. Figure 2 shows a sample of the playbook GUI for the robot deployment task. The GUI is written in Java to operate in a web browser. In the upper left corner, the GUI displays the list of available plays, organized in a browser metaphor. The user can select and execute plays at any level of the browser structure, thus providing more- or less-detailed commands to the robot team. In the lower left corner, the GUI shows status information for a selected robot. The right hand side displays the map, the home base (triangle), the robots' current locations (filled circles), and the robots' goal locations (clear circles). After the user selects a play, he can choose either Execute, and the MACBeth planner will generate a plan and send it to the executive, or Tailor, and MACBeth will generate a (constraint-free) plan, and then allow the user to set constraints and otherwise tailor the play to the current situation. To support the playbook commanding metaphor and allow the user to command at various levels of abstraction, the GUI interacts with the MACBeth planner to build complete robot team plans. MACBeth is a constraint-based tactical planning engine for multi-agent teams. MACBeth is designed for domains in which a human user must quickly specify a mission to a team of autonomous agents. In these domains, \"puzzle-mode\" thinking to come up with novel plans is not important; the key task is to rapidly and accurately tailor existing plans to novel situations. To this end, MACBeth combines hierarchical task network (HTN) planning and modern constraint reasoning techniques in a mixed-initiative planning system. The playbook GUI uses MACBeth to generate, check, and modify team plans. In this paradigm, \"calling a play\" means that the user declares that a certain task needs to be accomplished. The user can \"tailor\" the play by setting parameters and adding constraints specified by operating conditions. Tailoring a play essentially allows for adjustable autonomy on the robots: The user can command the robot team at different levels of detail, leaving unspecified details up to the judgment of the autonomous systems, when desired. For the TMR task, the target user is a soldier squad leader or robot operator within a squad. The plays contain internal timing constraints, and certain actions (e.g., take a photo) have related hardware requirements. Most remaining constraints are map- or environment-related, including navigation waypoints and goal locations (specified by the user) and required locomotion capabilities (specified by the route planner in the navigation layer). In tactical applications, the mission planning system must ensure that plans are feasible. If the user attempts to specify an infeasible plan, conflicts and inconsistencies should flagged by the planning algorithm as soon as possible, so that the user can retract choices or reconcile conflicting objectives early in the planning process. For example, a user might request a team of robots to map an area that is too large for them to cover in the required time. The planner should identify the inconsistency quickly, ideally before it expends planning effort on low-level details, and immediately alert the user, so that he may either modify his objectives or assign additional resources. The combination of HTN planning, constraint programming, and playbook GUI was chosen to meet these needs. Hierarchical task nets provide a representation for plans that is relatively easy for people to understand. They can also support very efficient planning, when the ability to construct novel plans (completeness) is less of interest. The HTN plans provide a skeleton on which to perform constraint reasoning, and form a very natural interface to the executive (see Section 5). Constraints on MACBeth's plans capture the complex relationships between (otherwise unrelated) tasks, allowing MACBeth to reason about the tradeoffs of different resource allocations. MACBeth's constraint handling also assists human planners in resource management and in appropriately assigning mission roles to team members. Thus, users do not have to reason about which agent does which task, or how to reallocate agents in order to achieve mission goals. Constraints also provide a clean interface to external knowledge sources. For instance, in this application the navigation layer provides route planning information based on a metric map. The route planner is interfaced to MACBeth via constraints: MACBeth does not understand the geometric character of routes, so the route planner gives it summary information in the form of constraints, such as Figure 2: Playbook GUI for TMR Deployment Domain the time the route is expected to take and the length of the route. MACBeth uses this information to make decisions about which routes to take. For example, the user may task two robots to deploy to two different locations, but not specify which robot goes to which location. In this situation, MACBeth fills in these missing details by querying the route planner for routes for each robot to each destination, and then chooses the lowest-cost robot-to-destination assignment according to the constraints calculated by the route planner. 5 Executive The executive dispatches tasks and coordinates the multiple robots. It acts as a bridge between the high-level symbolic task planner and the lower-level navigation layer. When the user chooses to execute a play, MACBeth sends the hierarchical task net to the executive using the generic Plan Representation Language. PRL defines each task in terms of parameters, subtasks, and temporal constraints between subtasks. MACBeth transmits the complete hierarchy, rather than just the leaf tasks, for several reasons. First, if the executive knows the hierarchical relationships between tasks, it can more flexibly report on status and can terminate complete subtrees with a single command. Second, as described below, some tasks must be augmented to cope with interference and other aspects of the real world that the planner does not model. Thus, the executive potentially needs to know about all the tasks in the hierarchy. Third, this allows for a flexible boundary between planner and executive. The planner can decide to plan some tasks at a high level of abstraction and to expand some to a lower level, while relying on the executive to fill in the details. The primary role of the executive is to execute plans according to the constraints imposed by the planner. The executive keeps track of the synchronization constraints between tasks and dispatches tasks when the constraints are satisfied. The executive is implemented using the Task Description Language [18]. TDL is an extension of C++ that contains explicit syntax to support task decomposition, task synchronization, execution monitoring, and exception handling. The TDL compiler transforms TDL programs into pure C++ code, plus calls to a general-purpose task management library. The transformed programs can then be compiled and linked with a normal C++ compiler. One feature of TDL is the expressive constructs it supports for specifying temporal constraints between tasks, including both qualitative and quantitative relationships between the start and end points of tasks. For instance, a TDL task can be constrained to precede another (meaning that it, and all of its subtasks, must complete before the other task, or any of its subtasks, can start), or a task can be constrained to start 10 seconds after another, or a task can be constrained to terminate whenever another task completes. While TDL was originally developed to support coordination amongst multiple behaviors on a single robot, we have found it to be very useful for synchronizing the tasks of multiple robots, as well. We are currently working to distribute TDL so that each robot can have its own executive that transparently coordinates with the other executives. Both MACBeth and the executive view multi-robot deployment as having the same basic form: For N \"stages\" (where N is the number of robots, which equals the number of locations), move some group of n robots to some set of m locations. To a first approximation, the various deployment strategies differ with respect to the number of robots moving per stage, and the order in which they move. For example, Figure 3 shows TDL code (simplified for clarity) for the three deployment strategies that we have implemented (see Section 2). \"deployList\" is an N-element array, where the i th element of the array indicates which robot is to end up in which location. In all the deployment strategies, no matter how the robots move initially, they all must end up in the locations specified by the \"deployList\". Group deployment is distinguished by the fact that the number of robots moving decreases over time, starting with N and ending with one. In contrast, the number of moving robots increases in the wave deployment. In the third approach, leap-frog deployment, the number of robots moving is constant (one). The deployment strategies in Figure 3 essentially treat each robot as a point object, ignoring potential interference from other robots. For instance, the group deployment strategy has robots moving to the same location. In the real world, we need to take account of potential interference caused by robots with volume and mass. While the local obstacle avoidance behaviors of the navigation layer ensure that the robots do not crash into each other, the emergent behavior may not be very efficient. To remedy this, we augment each of the basic deployment strategies with coordination behaviors that act to reduce interference. Interference in the group deployment strategy comes from two sources. First, in each stage the strategy is supposed to deploy a group of robots to a given location. Since real robots cannot occupy the same space, the executive actually tasks them with slightly different destinations. The robot that is supposed to end up at that location is sent directly to the location specified in the \"deployList\". The other robots are deployed along a line between that location and the next goal location, separated by a fixed distance. The other type of interference is that the robots move concurrently, starting from the same general location. To minimize the likelihood that they have to avoid one another, we stagger the robots. Each robot monitors the position of the next robot in the \"deployList\" and does not start moving until that robot is a given distance ahead of it. This combination of coordination strategies keeps the team of robots well disciplined (see Figure 5). Interference in the leap-frog deployment strategy occurs when one robot moves past other robots on its way to a goal location. This is exacerbated because we typically operate in narrow corridors. To minimize such interference, the already-deployed robots monitor the position of the moving robot. When it starts approaching, the robots move away (roughly perpendicular to the route of the moving robot), wait until the robot has passed, then move back into position. In this way, the designated deployment locations are left unoccupied for a minimal period of time. For the wave deployment strategy, there is no robot-robot interference, so no augmentation is necessary. One additional role of the executive is to send MACBeth and the GUI dynamic state information. After each task completes (or if it fails), the executive sends a status message back to the planner and GUI. It also monitors, and periodically transmits, the position and status of each robot. The GUI can synchronously request map information, which is used to display where the robots are, and the planner can request route information between arbitrary locations, which it uses to decide how to deploy the robots. Finally, the GUI can request camera images from particular robots, to give the user a robot-eye-view of the situation. 6 Navigation Our robots employ a probabilistic system for navigation and localization. At the beginning of the coordinated deployment, we assume that a map of the environment is readily available, such as the one shown in Figure 4. Maps like these are easily built using the software approach described in [20], which extends Elfes' and Moravec's occupancy grid mapping algorithm [8, 14] by a real-time method for concurrent localization. The map shown in Figure 4 was acquired by a single robot; the issue of multi-robot coordination during mapping is beyond the scope of this paper and approaches can be found in [5, 19]. Equipped with such an occupancy grid map, the navigation task can be decomposed into three components: localization, motion planning, and collision avoidance. Such a decomposition is a de-facto standard in mobile robotics: Localization is a pure observer, estimating robot position based on sensor data, without impacting the way the robot behaves. The control problem is decomposed into two components: The motion planner generates globally near-optimal paths, and the collision avoidance algorithm is responsible for generating motor commands that meet the dynamical constraints imposed by the physical robot. In brief, our localization module implements the Monte Carlo localization (MCL) algorithm [7], a Bayesian solution for the localization problem. This algorithm estimates a robot's position, denoted x, as a posterior, using the following recursive estimator: Here y is the most recent sensor measurement and u is the controls (or, alternatively, a differential odometry reading). The conditional probabilities are easily modeled using simple mixture models, as described in [10]. The MCL algorithm implements this equation using a particle filter [16], which generates a set of (weighted) random particles that, as a whole, represent the desired Figure 3: Deployment Strategies in TDL (Simplified) Goal GroupDeploy (DEPLOY_PTR deployList) { with (serial) { for (int i=0; i<length(deployList); i++) { spawn GroupDeploySub(i, deployList); } } } Goal GroupDeploySub (int phase, DEPLOY_PTR deployList) { with (parallel) { for (int j=phase; j<length(deployList); j++) { spawn Deploy(deployList[j].robot, deployList[phase].location); } } } Goal WaveDeploy (DEPLOY_PTR deployList) { with (serial) { for (int i=0; i<length(deployList); i++) { spawn WaveDeploySub(i, deployList); } } } Goal WaveDeploySub (int phase, DEPLOY_PTR deployList) { with (parallel) { for (int j=0, k=length(deployList)-1; j<=phase; j++) { spawn Deploy(deployList[k-phase+j].robot, deployList[j].location); } } } Goal LeapFrogDeploy (DEPLOY_PTR deployList) { with (serial) { for (int i=0; i<length(deployList); i++) { spawn LeapDeploySub(i, deployList); } } } Goal LeapDeploySub (int phase, DEPLOY_PTR deployList) { with (serial) { for (int j=0; j<=phase; j++) { spawn Deploy(deployList[phase].robot, deployList[j].location); } } } Bel x ( ) P y x ( ) , ( )Bel dx ( ) = posterior Bel(x). The basic algorithm is very straightforward: . Generate a set of random x' from Bel(x'). . For each x', guess x according to P(x | u, x'). . Weight each x numerically by P(y | x). These weights are taken into account in the next iteration. As argued in [7], this algorithm is extremely efficient, robust, and accurate. The path planner uses value iteration [3], a version of dynamic programming, to determine the shortest path to the goal. Value iteration computes the distance to a goal point from arbitrary starting locations, hence is capable of recovering from unanticipated motions generated by the collision avoidance module. For computational reasons, our approach considers two dimensions only, ignoring important factors such as inertia, torque limits, and obstacles (such as people) not present in the map. Those other factors are taken into account by DWA, the collision avoidance algorithm [9]. DWA computes motor commands from the motion plans and a set of hard and soft constraints. Hard constraints are necessary to ensure the safety of the robot and its environment. They make the robot avoid configurations that inevitably would lead to a collision. For example, hard constraints force the robot to decelerate when approaching an obstacle, so that a safe stop can be guaranteed at all times. Soft constraints express preferences in control space. Currently, DWA combines three soft constraints: the desire to move towards a goal location (as set forth by the motion planner), the desire to move fast, and the desire to stay clear of obstacles. Together, these three soft constraints lead to smooth and effective behavior. Under the name of BeeSoft, this navigation system has been distributed by RWI Inc., a major robot manufacturer, and is now in widespread use in academic institutions world-wide. It has been ported to a large number of mobile robot platforms, including Pioneer I and II (by ActivMedia), ISR Pioneer AT, B14, B18, B21, Nomad Scout, Superscout, and XR 4000, and also was at the core of two museum tour-guide robots. 7 Experience The multi-robot deployment scenario was tested in several locations, including a remote TMR demonstration. Most of our testing was done in the corridors of our building at Carnegie Mellon (Figures 4 and 5). A typical scenario used four robots: Robin and Marion, two Pioneer AT's modified to hold a SICK laser range finder, Amelia, an RWI B21 with a SICK laser, and Xavier, an RWI B24 that used only sonar sensors. Robin, Marion and Amelia all use the BeeSoft navigation package described in Section 6. Since Xavier uses a somewhat different system [17], we added a layer that translates executive commands into Xavier's formats. However, the executive still has to explicitly deal with the fact that some robots have different map representations, different sensors, and that some do not have cameras. A typical scenario was to have all four robots execute one group deployment stage to take them from the \"base\" (our lab, labeled A in Figure 4) to the intersection labeled B. Then, two robots would leap-frog deploy to locations C and D, and the other two would concurrently group deploy to E and F. The whole deployment would take about 3-4 minutes, and the robots would average about 50 meters of travel. After deployment, the robots executed a set play to return to base (in a concurrent, but uncoordinated, fashion). One problem with this scenario is that the first leap-frogging robot could conceivably interfere with the two robots performing the group deployment. While we could have augmented the plan to explicitly coordinate these robots, it was far easier (and nearly as robust) just to make sure they were ordered appropriately after the first group deployment (in particular, the leap-frogging robot should be ahead of the others). Figure 4: Learned Map of Part of Wean Hall at CMU A B C F E D Figure 5: Coordinated Deployment of Robin, Marion, Amelia and Xavier Our testing indicates that the coordinated strategies were effective and efficient. By the end of our testing, the strategies were also quite robust. This was mainly due to the explicit coordination behaviors (described in Section 5) that were added to alleviate possible interference between robots. 8 Conclusions To be useful in many applications, mobile robots need to be fairly autonomous and easy to control. This is even more important for teams of robots, especially in situations where the robots can potentially interfere with one another. This paper has described an integrated multi-robot system that (1) uses an intuitive \"playbook\" interface to simplify user control and understanding, (2) plans complex tasks scenarios, (3) uses explicit synchronization techniques to coordinate and monitor the behaviors of multiple, heterogeneous robots, and (4) uses probabilistic techniques to reliably and efficiently navigate the robots. The system has been tested in the area of deployment of teams of robots. Qualitatively different deployments were developed based on small syntactic changes to a basic underlying strategy of deploying N robots to N locations in N separate stages. This not only facilitates development, it also made it easier for users to switch strategies dynamically. Subsequent work has extended the system to multi-robot mapping and exploration [5, 19] and a similar system is being developed for multi-robot assembly and construction [11]. Acknowledgments Thanks to Greg Armstrong for maintaining all the robots and assisting with the experiments. This work is sponsored in part by DARPA via TACOM contract DAAE07-98-C-L032.", "authorlist": {"__class__": "tuple", "__value__": ["Reid Simmons", "David Apfelbaum", "Dieter Fox", "Robert P Goldman", "Karen Zita Haigh", "David J Musliner", "Michael Pelican", "Sebastian Thrun"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Reid Simmons", "David Apfelbaum", "Dieter Fox", "Robert P Goldman", "Karen Zita Haigh", "David J Musliner", "Michael Pelican", "Sebastian Thrun"]}, "institution": null, "title": "Coordinated Deployment of Multiple, Heterogeneous Robots", "journal": null, "abstract": "Abstract To be truly useful, mobile robots need to be fairly autonomous and easy to control. This is especially true in situations where multiple robots are used, due to the increase in sensory information and the fact that the robots can interfere with one another. This paper describes a system that integrates autonomous navigation, a task executive, task planning, and an intuitive graphical user interface to control multiple, heterogeneous robots. We have demonstrated a prototype system that plans and coordinates the deployment of teams of robots. Testing has shown the effectiveness and robustness of the system, and of the coordination strategies in particular.", "author-in-focus": "Sebastian Thrun", "cluster_name": "SebastianThrun", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9408227, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Sebastian Thrun", "Knut Mller", "Alexander Linden"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Sebastian Thrun", "Knut Mller", "Alexander Linden"]}, "institution": null, "title": "Adaptive Look-Ahead Planning", "journal": null, "abstract": null, "author-in-focus": "Sebastian Thrun", "cluster_name": "SebastianThrun", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.9408227, "email": null}]}, {"__class__": "tuple", "__value__": [{"body": null, "authorlist": {"__class__": "tuple", "__value__": ["Gabriel E Soto", "Stephen J Young", "Maryann E martone", "Thomas J Deerinck", "Stephan Lamont", "bridget O Carragher", "Kiyoshi Hamma", "Mark H ellisman"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["Gabriel E Soto", "Stephen J Young", "Maryann E martone", "Thomas J Deerinck", "Stephan Lamont", "bridget O Carragher", "Kiyoshi Hamma", "Mark H ellisman"]}, "institution": null, "title": "Serial section electron tomography: A method for three-dimensional reconstruction of large structures", "journal": "Neuroimage,year: 1994", "abstract": null, "author-in-focus": "Stephen J Young", "cluster_name": "StephenJYoung", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 0.97744435, "email": null}, {"body": null, "authorlist": {"__class__": "tuple", "__value__": ["M Hadida-Hassan", "S Young"]}, "alt-authorlist": {"__class__": "tuple", "__value__": ["M Hadida-Hassan", "S Young"]}, "institution": null, "title": "and et al. Web-based telemicroscopy", "journal": "Journal of Structural Biology,year: 1999", "abstract": null, "author-in-focus": "S Young", "cluster_name": "StephenJYoung", "altTitle": null, "editor": null, "year": null, "keyword": null, "author-in-focus-score": 1.0, "email": null}]}]}