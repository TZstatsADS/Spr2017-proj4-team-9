---
title: "Project 4 - Paper 5 Main Script"
author: "Xuehan Liu"
date: "04/05/2017"
output: pdf_document
---

This file is currently a template for implementing one of the suggested papers, Han, Zha, & Giles (2005). Due to the nature of the method, we only implement the method on a subset of the data, "AKumar.txt". In your project, you need to work on the whole dataset. You should follow the same structure as in this tutorial, but update it according to the papers you are assigned.

## Step 0: Load the packages, specify directories

```{r}
setwd("/Users/xuehan/Desktop/Spr2017-proj4-team-9/")
# here replace it with your own path or manually set it in RStudio
# to where this rmd file is located


if (!require("pacman")) install.packages("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
```

## Step 1: Load and process the data

```{r}
AKumar <- data.frame(scan("../data/nameset/AKumar.txt",
                          what = list(Coauthor = "", Paper = "", Journal = ""),
                          sep=">", quiet=TRUE),stringsAsFactors=FALSE)
# This need to be modified for different name set

#train<-function(dataset,lambda){
########change all AKumar to dataset

# extract canonical author id befor "_"
AKumar$AuthorID <- sub("_.*","",AKumar$Coauthor)
# extract paper number under same author between "_" and first whitespace
AKumar$PaperNO <- sub(".*_(\\w*)\\s.*", "\\1", AKumar$Coauthor)
# delete "<" in AKumar$Coauthor, you may need to further process the coauthor
# term depending on the method you are using
AKumar$Coauthor <- gsub("<","",sub("^.*?\\s","", AKumar$Coauthor))
# delete "<" in AKumar$Paper
AKumar$Paper <- gsub("<","",AKumar$Paper)
# add PaperID for furthur use, you may want to combine all the nameset files and 
# then assign the unique ID for all the citations
AKumar$PaperID <- rownames(AKumar)




## Step 2: Feature Design

it_train <- itoken(AKumar$Paper, 
             preprocessor = tolower, 
             tokenizer = word_tokenizer,
             ids = AKumar$PaperID,
             # turn off progressbar because it won't look nice in rmd
             progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                   "at", "of", "above", "under"))

#vocab

vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)

tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)


## Step 3: Implementing hierirchical clustering and training parameters 

#In this section, our goal is to train the lamda on hierirchical clustering on our text file by the error driven online training method in the paper5. We use the ranking perceptron to update the parameters.


####Initialize Parameter lambda
lambda<-rep(1,ncol(dtm_train_tfidf))

#Add the Author's ID as the label column to the feature matrix for future use
dtm_train_tfidf<-cbind(dtm_train_tfidf,as.numeric(AKumar$AuthorID))

#Compute distance between two rows (pair of featured citations) 
distance<-matrix(NA,nrow=nrow(dtm_train_tfidf[,-1]),ncol=nrow(dtm_train_tfidf[,-1]))


for (i in 1:nrow(dtm_train_tfidf[,-1])){
  for (j in 1:nrow(dtm_train_tfidf[,-1])){
      distance[i,j]<-sum((dtm_train_tfidf[i,-1]-dtm_train_tfidf[j,-1])^2)
  }
}

#Set diagonal element to a very large number because the diagonal entries represents two same documents
for(i in 1:nrow(dtm_train_tfidf[,-1])) {
  distance[i,i] <- 999
}

row.ind<-vector(length=length(which(distance==0))/2)
col.ind<-vector(length=length(which(distance==0))/2)          

for (m in 1: length(which(distance==0))/2){
  row.ind[m]<-floor(which(distance==0)[m]/244)+1
  col.ind[m]<-5214-(row[m]-1)*244
  if (dtm_train_tfidf[row.ind[m],nrow(dtm_train_tfidf)-1]==dtm_train_tfidf[col.ind[m],nrow(dtm_train_tfidf)-1]){
    S[m]<-dist(dtm_train_tfidf[row.ind[m],-1],dtm_train_tfidf[row.ind[m],-1])
  }
  else{
    print(dtm_train_tfidf[row.ind[m]],dtm_train_tfidf[col.ind[m]])
  }
}

row.ind
col.ind

#Check whether the two paper share the same author

compare.author.id<-vector
for (a in 1:length(which(distance==0))/2){
  compare.author.id[a]<-dtm_train_tfidf[row.ind[a],nrow(dtm_train_tfidf)-1]==dtm_train_tfidf[col.ind[a],nrow(dtm_train_tfidf)-1]
}



#Based on the paper, we define our own score S by the negative of the distance, because high score=>high accuracy
#Find the pair with highest S and group them together
S<-(-distance)
ind<-matrix(which(S == max(S),arr.ind=T),ncol=2,byrow=F)
ind<-ind[ind[,1]!=ind[,2]]
S[which.max(S)]

#Find the ture score S*:=accuracy 

S_star<-sum(dtm_train_tfidf[,ncol(dtm_train_tfidf)]==dtm_train_tfidf[,ncol(dtm_train_tfidf)])

while(){
  
}

#Update parameters
lambda<-lambda+ifelse(T,1,0)+(F(n_hat)-F(n_star))


}
```





##First try

Given the training set, we are able to generate the true clusters. Based on the paper, we define true score S_star as the distance of the sum of clusterwise distance. We define our own score function as the sum of distance of clusters that is clustered by hclust() function.

```{r}
#Compute the true score S_star for the giving training data

element<-list()
S_star<-vector(length=length(unique(AKumar$AuthorID)))
for (i in 1:length(unique(AKumar$AuthorID))){
  element[[i]]<-dtm_train_tfidf[dtm_train_tfidf[,ncol(dtm_train_tfidf)]==i,]
  S_star[i]<-sum(dist(element[[i]]))/2
}
S_star<-mean(S_star)



while (){
  
#Implement Hierirchical Clustering 
h<-hclust(dist(dtm_train_tfidf*lambda))
#Check the result for the number of cluster equals to the number of unique authors in the dataset. 
h_result<-cutree(h,k=length(unique(AKumar$AuthorID)))

#Compute the our own score function S
S<-vector(length=length(unique(AKumar$AuthorID)))
element_s<-list()
for (i in 1:length(unique(AKumar$AuthorID))){
  element_s[[i]]<-dtm_train_tfidf[which(h_result==i),]
  S[i]<-sum(dist(element_s[[i]]))/2
}
S<-mean(S)


lambda<-lambda-1*((S_star-S)/S_star)

}
```

